---
title: "WHO, Covid-19 situation report"
author: "Mitsuo Shiota"
date: "2020/3/7"
output: 
  github_document:
    toc: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Updated: `r Sys.Date()`

I added "USA, Covid-19 situation by state" in [another page](USA.md).

## Summary

<https://mitsuoxv.shinyapps.io/covid/>

Coronavirus is affecting the world economy. Uncertaintiy is very high. I searched around and found some informative sites, like [Coronavirus Situation Dashboard](https://who.maps.arcgis.com/apps/opsdashboard/index.html#/c88e37cfc43b4ed3baf977d77e4a0667) and [Coronavirus Update by worldometer](https://www.worldometers.info/coronavirus/). But they fail to offer time-series data of the newly confirmed cases by each area, in which I am most interested. If the average number of infections one infected person inflict is even slightly more than one, infections grow exponentially. If less than one, the newly confirmed cases begin to decrease, and the virus will be contained eventually in that area.

Note that the confirmed cases are not the actual cases, due to delays from infection to symptoms, limited testing capacity, and so on, as [Nate Silver tells us](https://fivethirtyeight.com/features/coronavirus-case-counts-are-meaningless/).

I later found [Johns Hopkins University, Coronavirus Resource Center](https://coronavirus.jhu.edu/) and [Financial Times, Coronavirus tracked](https://www.ft.com/content/a26fbf7e-48f8-11ea-aeb3-955839e06441) are very informative, and that they provide some time-series charts of the newly confirmed cases.

I added the United States page to [my Shiny App](https://mitsuoxv.shinyapps.io/covid/) on May 25, 2020. I use data from [USAFacts page](https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/).

```{r library, include=FALSE}
library(tidyverse)
library(tsibble)
library(tqr)
library(rvest)
library(readxl)
library(scales)
```

## Read data from WHO

WHO offers [Data Table in its Dashboard](https://covid19.who.int/table). I use the data from https://covid19.who.int/WHO-COVID-19-global-data.csv.

I also use the data, like area name and population, from https://countrycode.org/.

```{r read_from_WHO, echo=FALSE}
# Read csv from url
WHO_COVID_19_global_data <- read_csv("https://covid19.who.int/WHO-COVID-19-global-data.csv",
                        col_types = cols(Date_reported = col_date(format = "%Y-%m-%d")),
                        na = c(""))

table2 <- WHO_COVID_19_global_data %>% 
  select(Date_reported, Country_code, Country, New_cases, Cumulative_cases, New_deaths, Cumulative_deaths)

names(table2) <- c("publish_date", "iso_alpha2", "area", "new_conf", "cum_conf", "new_deaths", "cum_deaths")

# Add info to table2
cc_html <- read_html("https://countrycode.org/")

cc_df <- cc_html %>% 
  html_nodes("table") %>% 
  .[[1]] %>% 
  html_table()

cc_df$POPULATION <- as.numeric(str_remove_all(cc_df$POPULATION, ","))

cc_df <- cc_df %>% 
  mutate(
    iso_alpha2 = str_sub(`ISO CODES`, 1L, 2L)
  )

UNSD_Methodology <- read_excel("data/UNSD-Methodology.xlsx")

cc_df <- cc_df %>% 
  left_join(UNSD_Methodology, by = c("iso_alpha2" = "ISO-alpha2 Code"))

cc_df <- cc_df %>% 
  mutate(
    region = if_else(`Sub-region Name` %in% c("Southern Europe", "Northern Europe", "Western Europe"), "West Europe", 
                     if_else(`Sub-region Name` %in% c("Australia and New Zealand", "Melanesia", "Micronesia", "Polynesia"), "Oceania", `Sub-region Name`)
    )
  )

# Change area name according to countrycode.org
table2 <- table2 %>% 
  left_join(cc_df, by = "iso_alpha2") %>% 
  mutate(COUNTRY = if_else(is.na(COUNTRY), area, COUNTRY)) %>% 
  select(-area) %>% 
  rename(area = COUNTRY)

```

## Newly confirmed cases by region

I watch newly confirmed cases. China is suceeding to contain the coronavirus, but areas outside China now face the challenge.

Region classification is mostly based on [UN M49 Standard](https://unstats.un.org/unsd/methodology/m49/). One exception is "West Europe", which I make by combining "Western", "Northern" and "Southern Europe". "West Europe" is basically Europe other than "Eastern Europe".

```{r chart, echo=FALSE, warning=FALSE}
table2 %>% 
  drop_na(region) %>% 
  group_by(publish_date, region) %>% 
  summarize(new_conf = sum(new_conf, na.rm = TRUE), .groups = "drop_last") %>% 
  as_tsibble(key = region, index = publish_date) %>% 
  tq_ma(n = 7) %>% 
  ggplot(aes(publish_date, new_conf,
             color = fct_reorder2(region, publish_date, new_conf))) +
  geom_hline(yintercept = 0, color = "white", size = 2) +
  geom_line(size = 1) +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Confirmed cases (new)",
    x = "published date (7 days average)",
    y = NULL,
    caption = str_c("Latest: ", max(table2$publish_date)),
    color = NULL
  ) +
  theme(plot.title = element_text(size = rel(2)))
```

## Fastest spreading areas

Among areas with more than 5 million population, highest "speed_since_100",  which is average number of newly confirmed cases per day since cumulative cases became more than 100, are:

```{r per_capita, echo=FALSE}
areas_5m <- table2 %>% 
  filter(POPULATION >= 5000000)

areas_5m %>% 
  group_by(area) %>% 
  arrange(publish_date) %>% 
  filter(cum_conf > 100) %>% 
  mutate(
    days_since_100 = row_number(publish_date),
    speed_since_100 = (cum_conf - min(cum_conf)) / days_since_100
    ) %>% 
  ungroup() %>% 
  filter(publish_date == max(publish_date)) %>% 
  arrange(desc(speed_since_100)) %>% 
  select(area, speed_since_100, cum_conf, days_since_100) %>% 
  head(20)
```

Above calculation might be unfair to populous areas. Below "per_capita_cum_conf" is cumulative cases per 1 million population. Highest "speed_std_since_100", which is per day growth of cumulative cases per 1 million population since cumulative cases became more than 100, are:

```{r standardized, echo=FALSE}
areas_5m %>% 
  group_by(area) %>% 
  arrange(publish_date) %>% 
  filter(cum_conf > 100) %>% 
  mutate(
    days_since_100 = row_number(publish_date),
    per_capita_cum_conf = cum_conf * 1000000 / POPULATION,
    speed_std_since_100 = (cum_conf - min(cum_conf)) * 1000000 / POPULATION / days_since_100
    ) %>% 
  ungroup() %>% 
  filter(publish_date == max(publish_date)) %>% 
  arrange(desc(speed_std_since_100)) %>% 
  select(area, speed_std_since_100, per_capita_cum_conf, days_since_100) %>% 
  head(20)
```


## Highest fatality rate areas

Among areas with more than 5 million population and more than 10 cumulative deaths, highest "fatality_rate", which is cumulative deaths per 100 cumulative confirmed cases, are:

```{r fatality_rates, echo=FALSE}
areas_5m %>% 
  filter(
    cum_deaths > 10,
    publish_date == max(publish_date)
    ) %>% 
  mutate(fatality_rate = cum_deaths * 100 / cum_conf) %>% 
  arrange(desc(fatality_rate)) %>% 
  select(area, fatality_rate, cum_deaths, cum_conf) %>% 
  head(20)
```

## Highest deaths per population areas

Among areas with more than 5 million population, highest "deaths_per_1m", which is cumulative deaths per 1 million population, are:

```{r deaths_per_population, echo=FALSE}
areas_5m %>% 
  filter(
    publish_date == max(publish_date)
    ) %>% 
  mutate(
    pop_mil = POPULATION / 1000000,
    deaths_per_1m = cum_deaths / pop_mil
    ) %>% 
  arrange(desc(deaths_per_1m)) %>% 
  select(area, deaths_per_1m, cum_deaths, pop_mil) %>% 
  head(20)
```

## Save data

```{r save_region, echo=FALSE}
table2a <- table2 %>% 
  replace_na(list(region = "Others"))

table2a %>% 
  group_by(publish_date, region) %>% 
  summarize(new_conf = sum(new_conf, na.rm = TRUE), .groups = "drop_last") %>% 
  as_tsibble(key = region, index = publish_date) %>% 
  tq_ma(n = 7) %>% 
  pivot_wider(names_from = region, values_from = new_conf, values_fill = 0) %>% 
  write.csv("data/new_conf_7ma_by_region.csv", row.names = FALSE)

table2a %>% 
  group_by(publish_date, region) %>% 
  summarize(new_deaths = sum(new_deaths, na.rm = TRUE), .groups = "drop_last") %>% 
  as_tsibble(key = region, index = publish_date) %>% 
  tq_ma(n = 7) %>% 
  pivot_wider(names_from = region, values_from = new_deaths, values_fill = 0) %>% 
  write.csv("data/new_deaths_7ma_by_region.csv", row.names = FALSE)

```

```{r save_europe, echo=FALSE}
europe <- table2 %>% 
  filter(iso_alpha2 %in% c("FR", "DE", "IT", "ES", "GB"))

europe %>% 
  group_by(publish_date, area) %>% 
  summarize(new_conf = sum(new_conf, na.rm = TRUE), .groups = "drop_last") %>% 
  as_tsibble(key = area, index = publish_date) %>% 
  tq_ma(n = 7) %>% 
  pivot_wider(names_from = area, values_from = new_conf, values_fill = 0) %>% 
  write.csv("data/new_conf_7ma_europe.csv", row.names = FALSE)

europe %>% 
  group_by(publish_date, area) %>% 
  summarize(new_deaths = sum(new_deaths, na.rm = TRUE), .groups = "drop_last") %>% 
  as_tsibble(key = area, index = publish_date) %>% 
  tq_ma(n = 7) %>% 
  pivot_wider(names_from = area, values_from = new_deaths, values_fill = 0) %>% 
  write.csv("data/new_deaths_7ma_europe.csv", row.names = FALSE)

```

You can see the data I use in table2.csv in data directory of this repository.

```{r save, echo=FALSE}
table2 %>% 
  write.csv("data/table2.csv", row.names = FALSE)

table2 <- table2 %>% 
  select(publish_date, area, new_conf, new_deaths, cum_conf, cum_deaths)

saveRDS(table2, "data/table2.rds")
```

EOL

